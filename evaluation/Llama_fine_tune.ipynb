{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmonuYjEqnCY"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install peft\n",
        "!pip install ipywidgets\n",
        "!jupyter labextension install @jupyter-widgets/jupyterlab-manager"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login('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')"
      ],
      "metadata": {
        "id": "SUeibQ5yquS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from typing import Dict, List"
      ],
      "metadata": {
        "id": "xSzJUvvFquWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SECDataProcessor:\n",
        "    def __init__(self, data_dir: str, eval_file: str):\n",
        "        self.data_dir = data_dir\n",
        "        self.eval_file = eval_file\n",
        "        self.special_tokens = {\n",
        "            \"company_start\": \"<|company|>\",\n",
        "            \"ticker_start\": \"<|ticker|>\",\n",
        "            \"filing_start\": \"<|filing|>\",\n",
        "            \"item_start\": \"<|item|>\",\n",
        "            \"question_start\": \"<|question|>\",\n",
        "            \"answer_start\": \"<|answer|>\",\n",
        "            \"sep\": \"<|sep|>\"\n",
        "        }\n",
        "\n",
        "    def process_10k_file(self, file_path: str) -> Dict:\n",
        "        \"\"\"Process a single 10-K JSON file.\"\"\"\n",
        "        with open(file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Extract relevant fields\n",
        "        text = f\"{self.special_tokens['company_start']}{data.get('company', '')}\"\n",
        "        text += f\"{self.special_tokens['ticker_start']}{data.get('cik', '')}\"\n",
        "        text += f\"{self.special_tokens['filing_start']}{data.get('filing_type', '')}\"\n",
        "\n",
        "        # Add items (you can add more items as needed)\n",
        "        for item_num in range(1, 8):\n",
        "            item_key = f'item_{item_num}'\n",
        "            if item_key in data:\n",
        "                text += f\"{self.special_tokens['item_start']}{item_key}: {data[item_key]}\"\n",
        "\n",
        "        return text\n",
        "\n",
        "    def prepare_training_data(self) -> Dataset:\n",
        "        \"\"\"Prepare training data from 10-K files.\"\"\"\n",
        "        texts = []\n",
        "\n",
        "        for filename in os.listdir(self.data_dir):\n",
        "            if filename.endswith('.json'):\n",
        "                file_path = os.path.join(self.data_dir, filename)\n",
        "                text = self.process_10k_file(file_path)\n",
        "                texts.append({\"text\": text})\n",
        "\n",
        "        return Dataset.from_list(texts)\n",
        "\n",
        "    def prepare_eval_data(self) -> Dataset:\n",
        "        \"\"\"Prepare evaluation data from CSV.\"\"\"\n",
        "        df = pd.read_csv(self.eval_file)\n",
        "        eval_texts = []\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            text = f\"{self.special_tokens['company_start']}{row['Company']}\"\n",
        "            text += f\"{self.special_tokens['question_start']}{row['Question']}\"\n",
        "            text += f\"{self.special_tokens['answer_start']}{row['Answer']}\"\n",
        "            eval_texts.append({\"text\": text})\n",
        "\n",
        "        return Dataset.from_list(eval_texts)"
      ],
      "metadata": {
        "id": "--TlPWSTquY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMFineTuner:\n",
        "    def __init__(self, model_name: str, special_tokens: Dict[str, str]):\n",
        "        self.model_name = model_name\n",
        "        self.special_tokens = special_tokens\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "\n",
        "    def preprocess_data(self, examples: Dict[str, List]) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Preprocess data for training.\"\"\"\n",
        "        if self.tokenizer is None:\n",
        "            raise RuntimeError(\"Tokenizer must be initialized first\")\n",
        "\n",
        "        # Tokenize the text directly\n",
        "        encodings = self.tokenizer(\n",
        "            examples['text'],\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encodings[\"input_ids\"],\n",
        "            \"attention_mask\": encodings[\"attention_mask\"],\n",
        "            \"labels\": encodings[\"input_ids\"].clone()\n",
        "        }\n",
        "\n",
        "    def prepare_dataset(self, dataset: Dataset) -> Dataset:\n",
        "        \"\"\"Prepare dataset with proper encodings.\"\"\"\n",
        "        # Apply preprocessing to create properly encoded dataset\n",
        "        encoded_dataset = dataset.map(\n",
        "            lambda examples: self.preprocess_data(examples),\n",
        "            batched=True,\n",
        "            remove_columns=dataset.column_names\n",
        "        )\n",
        "\n",
        "        return encoded_dataset\n",
        "\n",
        "    def setup_tokenizer_and_model(self):\n",
        "        \"\"\"Initialize and setup tokenizer and model.\"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "\n",
        "        # Add special tokens\n",
        "        special_tokens_dict = {'additional_special_tokens': list(self.special_tokens.values())}\n",
        "        self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "        # Setup LoRA\n",
        "        lora_config = LoraConfig(\n",
        "            r=16,\n",
        "            lora_alpha=32,\n",
        "            target_modules=[\"q_proj\", \"v_proj\"],\n",
        "            lora_dropout=0.05,\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\"\n",
        "        )\n",
        "        self.model = get_peft_model(self.model, lora_config)\n",
        "\n",
        "    def train(self, train_dataset: Dataset, output_dir: str):\n",
        "        \"\"\"Train the model.\"\"\"\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            num_train_epochs=3,\n",
        "            per_device_train_batch_size=4,\n",
        "            gradient_accumulation_steps=4,\n",
        "            learning_rate=2e-4,\n",
        "            fp16=True,\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_steps=100,\n",
        "            optim=\"adamw_torch\",\n",
        "            remove_unused_columns=False\n",
        "        )\n",
        "\n",
        "        train_dataset = self.prepare_dataset(train_dataset)\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            data_collator=DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "        trainer.save_model(output_dir)\n",
        "\n",
        "    def generate_answer(self, company: str, question: str) -> str:\n",
        "        \"\"\"Generate answer for a given question.\"\"\"\n",
        "        prompt = f\"{self.special_tokens['company_start']}{company}\"\n",
        "        prompt += f\"{self.special_tokens['question_start']}{question}\"\n",
        "        prompt += f\"{self.special_tokens['answer_start']}\"\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            **inputs,\n",
        "            max_length=1024,\n",
        "            temperature=0.7,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=self.tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "xugn-HOxquby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMFineTuner:\n",
        "    def __init__(self, model_name: str, special_tokens: Dict[str, str]):\n",
        "        self.model_name = model_name\n",
        "        self.special_tokens = special_tokens\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "\n",
        "    def preprocess_data(self, examples: Dict[str, List]) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Preprocess data for training.\"\"\"\n",
        "        if self.tokenizer is None:\n",
        "            raise RuntimeError(\"Tokenizer must be initialized first\")\n",
        "\n",
        "        # Tokenize the text directly\n",
        "        encodings = self.tokenizer(\n",
        "            examples['text'],\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encodings[\"input_ids\"],\n",
        "            \"attention_mask\": encodings[\"attention_mask\"],\n",
        "            \"labels\": encodings[\"input_ids\"].clone()\n",
        "        }\n",
        "\n",
        "    def prepare_dataset(self, dataset: Dataset) -> Dataset:\n",
        "        \"\"\"Prepare dataset with proper encodings.\"\"\"\n",
        "        # Apply preprocessing to create properly encoded dataset\n",
        "        encoded_dataset = dataset.map(\n",
        "            lambda examples: self.preprocess_data(examples),\n",
        "            batched=True,\n",
        "            remove_columns=dataset.column_names\n",
        "        )\n",
        "\n",
        "        return encoded_dataset\n",
        "\n",
        "    def setup_tokenizer_and_model(self):\n",
        "        \"\"\"Initialize and setup tokenizer and model.\"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "\n",
        "        # Add special tokens\n",
        "        special_tokens_dict = {'additional_special_tokens': list(self.special_tokens.values())}\n",
        "        self.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "        # Setup LoRA\n",
        "        lora_config = LoraConfig(\n",
        "            r=16,\n",
        "            lora_alpha=32,\n",
        "            target_modules=[\"q_proj\", \"v_proj\"],\n",
        "            lora_dropout=0.05,\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\"\n",
        "        )\n",
        "        self.model = get_peft_model(self.model, lora_config)\n",
        "\n",
        "    def train(self, train_dataset: Dataset, output_dir: str):\n",
        "        \"\"\"Train the model.\"\"\"\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            num_train_epochs=3,\n",
        "            per_device_train_batch_size=4,\n",
        "            gradient_accumulation_steps=4,\n",
        "            learning_rate=2e-4,\n",
        "            fp16=True,\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_steps=100,\n",
        "            optim=\"adamw_torch\",\n",
        "            remove_unused_columns=False\n",
        "        )\n",
        "\n",
        "        train_dataset = self.prepare_dataset(train_dataset)\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            data_collator=DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "        trainer.save_model(output_dir)\n",
        "\n",
        "    def generate_answer(self, company: str, question: str) -> str:\n",
        "        \"\"\"Generate answer for a given question.\"\"\"\n",
        "        prompt = f\"{self.special_tokens['company_start']}{company}\"\n",
        "        prompt += f\"{self.special_tokens['question_start']}{question}\"\n",
        "        prompt += f\"{self.special_tokens['answer_start']}\"\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            **inputs,\n",
        "            max_length=1024,\n",
        "            temperature=0.7,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=self.tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "fsPBCZokqueI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UTQ9EPCyqugi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling\n",
        "\n",
        "# Step 1: Load all JSON files from the directory\n",
        "def load_data_from_directory(directory_path):\n",
        "    all_texts = []\n",
        "    for file_name in os.listdir(directory_path):\n",
        "        file_path = os.path.join(directory_path, file_name)\n",
        "        if file_name.endswith(\".json\"):  # Only process JSON files\n",
        "            with open(file_path, 'r') as file:\n",
        "                data = json.load(file)\n",
        "                # Combine the relevant fields into a single text entry\n",
        "                text = \"\\n\".join([value for key, value in data.items() if isinstance(value, str)])\n",
        "                all_texts.append(text)\n",
        "    return all_texts\n",
        "\n",
        "# Directory containing JSON files\n",
        "directory_path = \"data/10-K\"\n",
        "all_texts = load_data_from_directory(directory_path)\n",
        "\n",
        "# Save combined texts to a single file\n",
        "fine_tune_text_file = \"fine_tune_data.txt\"\n",
        "with open(fine_tune_text_file, 'w') as f:\n",
        "    f.write(\"\\n\\n\".join(all_texts))  # Separate each file's content with a blank line\n",
        "\n",
        "# Step 2: Load Tokenizer and Model\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "\n",
        "# Step 3: Create Dataset\n",
        "def create_dataset(file_path, tokenizer, block_size=512):\n",
        "    dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=file_path,\n",
        "        block_size=block_size\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "dataset = create_dataset(fine_tune_text_file, tokenizer)\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "# Step 4: Fine-tuning\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./finetuned_model\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=16,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=500,\n",
        "    learning_rate=5e-5,\n",
        "    fp16=True,  # Use mixed precision for large models\n",
        "    eval_strategy=\"no\",\n",
        "    save_strategy=\"steps\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"scratch/finetuned_model\")\n",
        "tokenizer.save_pretrained(\"scratch/finetuned_model\")\n",
        "\n",
        "# Step 5: Evaluation\n",
        "eval_data_file = \"evaluation_dataset.csv\"\n",
        "eval_data = pd.read_csv(eval_data_file)\n",
        "\n",
        "# Ensure eval_data contains \"question\" column\n",
        "if \"Question\" not in eval_data.columns:\n",
        "    raise ValueError(\"The evaluation dataset must have a 'Question' column.\")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "complete_list = []\n",
        "k = 0\n",
        "\n",
        "for question in tqdm(eval_data[\"Question\"]):\n",
        "    # Tokenize input with padding and return attention mask\n",
        "    inputs = tokenizer(\n",
        "        question,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=200  # Set as per your model's context size\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Generate outputs with attention mask\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=200,\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        pad_token_id=tokenizer.pad_token_id\n",
        "    )\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    complete_list.append({\n",
        "        \"no\": k,\n",
        "        \"question\":question,\n",
        "        \"answer\": answer\n",
        "    })\n",
        "    k += 1\n",
        "    answers.append(answer)\n",
        "\n",
        "# Save results to CSV\n",
        "df = pd.DataFrame(complete_list)\n",
        "\n",
        "df.to_csv(\"scratch/llama-answers.csv\", index=False)\n",
        "\n",
        "print(\"Evaluation complete. Results saved to 'evaluation_results.csv'.\")\n"
      ],
      "metadata": {
        "id": "ZzZMk49gqui_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results to CSV\n",
        "df = pd.DataFrame(complete_list)\n",
        "\n",
        "df.to_csv(\"scratch/llama-answers.csv\", index=False)\n",
        "\n",
        "print(\"Evaluation complete. Results saved to 'evaluation_results.csv'.\")"
      ],
      "metadata": {
        "id": "Ijeg7TMvqulN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iph-KesbqunX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qH_ePLh2quqC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
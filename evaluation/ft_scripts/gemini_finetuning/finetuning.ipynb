{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API_KEY = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# Replace 'your-api-key-here' with your actual API key for GenAI\n",
    "API_KEY = \"your-api-key-here\" # Ayushi, I replaced this so nobody sees your API key :) \n",
    "\n",
    "# Initialize the GenAI client\n",
    "genai.configure(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f69f88d9994b0c8dd8920ca5bc15f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TunedModel(name='tunedModels/increment-x0nduqe11yoh',\n",
      "           source_model='models/gemini-1.5-flash-001-tuning',\n",
      "           base_model='models/gemini-1.5-flash-001-tuning',\n",
      "           display_name='increment',\n",
      "           description='',\n",
      "           temperature=1.0,\n",
      "           top_p=0.95,\n",
      "           top_k=64,\n",
      "           state=<State.ACTIVE: 2>,\n",
      "           create_time=datetime.datetime(2024, 11, 25, 12, 35, 50, 752843, tzinfo=datetime.timezone.utc),\n",
      "           update_time=datetime.datetime(2024, 11, 25, 14, 10, 41, 894410, tzinfo=datetime.timezone.utc),\n",
      "           tuning_task=TuningTask(start_time=datetime.datetime(2024, 11, 25, 12, 35, 55, 521090, tzinfo=datetime.timezone.utc),\n",
      "                                  complete_time=datetime.datetime(2024, 11, 25, 14, 10, 41, 894410, tzinfo=datetime.timezone.utc),\n",
      "                                  snapshots=[...],\n",
      "                                  hyperparameters=Hyperparameters(epoch_count=20,\n",
      "                                                                  batch_size=4,\n",
      "                                                                  learning_rate=0.001)),\n",
      "           reader_project_numbers=None)\n",
      "What can I help you with? Please provide me with more context or a specific question. For example, you could ask:\n",
      "\n",
      "* \"What is the capital of France?\"\n",
      "* \"Can you write a short poem about the ocean?\"\n",
      "* \"What is the meaning of life?\"\n",
      "\n",
      "I'm ready to assist you!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "base_model = \"models/gemini-1.5-flash-001-tuning\"\n",
    "\n",
    "\n",
    "with open(\"/Users/ayushimathur/Downloads/cs6220-project-eval_will/gemini_finetuning/training_data.json\") as f:\n",
    "    training_data = json.load(f)\n",
    "\n",
    "truncated_training_data = []\n",
    "\n",
    "for _ in training_data:\n",
    "    if len(_[\"output\"]) > 2000:\n",
    "        continue\n",
    "    else:\n",
    "        truncated_training_data.append(_)\n",
    "\n",
    "operation = genai.create_tuned_model(\n",
    "    # You can use a tuned model here too. Set `source_model=\"tunedModels/...\"`\n",
    "    display_name=\"increment\",\n",
    "    source_model=base_model,\n",
    "    epoch_count=20,\n",
    "    batch_size=4,\n",
    "    learning_rate=0.001,\n",
    "    training_data=truncated_training_data,\n",
    "    \n",
    ")\n",
    "\n",
    "for status in operation.wait_bar():\n",
    "    time.sleep(10)\n",
    "\n",
    "result = operation.result()\n",
    "print(result)\n",
    "# # You can plot the loss curve with:\n",
    "# snapshots = pd.DataFrame(result.tuning_task.snapshots)\n",
    "# sns.lineplot(data=snapshots, x='epoch', y='mean_loss')\n",
    "\n",
    "model = genai.GenerativeModel(model_name=result.name)\n",
    "result = model.generate_content(\"III\")\n",
    "print(result.text)  # IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"What can I help you with? Please provide me with more context or a specific question. For example, you could ask:\\n\\n* \\\"What is the capital of France?\\\"\\n* \\\"Can you write a short poem about the ocean?\\\"\\n* \\\"What is the meaning of life?\\\"\\n\\nI'm ready to assist you!\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"index\": 0,\n",
      "          \"safety_ratings\": [\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 2,\n",
      "        \"candidates_token_count\": 67,\n",
      "        \"total_token_count\": 69\n",
      "      }\n",
      "    }),\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "710it [07:53,  1.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "testing_outputs = []\n",
    "with open(\"/Users/ayushimathur/Downloads/cs6220-project-eval_will/gemini_finetuning/testing_data.json\") as f:\n",
    "    testing_data = json.load(f)\n",
    "for idx, _ in tqdm(enumerate(testing_data)):\n",
    "    try:\n",
    "        result = model.generate_content(_[\"text_input\"])\n",
    "        result = result.text\n",
    "    except:\n",
    "        result = \"\"\n",
    "    testing_output = {\n",
    "        \"prompt\": _[\"text_input\"],\n",
    "        \"gold_ans\": _[\"output\"],\n",
    "        \"response\": result\n",
    "    }\n",
    "    testing_outputs.append(testing_output)\n",
    "    if idx%500 == 499:\n",
    "        time.sleep(100)\n",
    "\n",
    "with open(\"/Users/ayushimathur/Downloads/cs6220-project-eval_will/gemini_finetuning/model_response.json\", \"w\") as f:\n",
    "    json.dump(testing_outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_hw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
